#!/usr/bin/env python3
"""
=============================================================================
LM STUDIO GPT-OSS FINE-TUNING MANAGER - M4 Max Optimized Training
=============================================================================

PURPOSE:
Fine-tune OpenAI's gpt-oss-20b model using LM Studio with MLX-LM optimization
for M4 Max hardware. Leverages Apple Silicon's unified memory and Metal 
Performance Shaders for maximum efficiency.

INPUT FILES:
- arjun_voice_training.jsonl - OpenAI format training dataset (80 pairs)
  * Premium Claude Opus processed investment voice data
  * Format: {"messages": [{"role": "system/user/assistant", "content": "..."}]}
  * High-quality analogies, data-driven insights, personal tone

- arjun_voice_validation.jsonl - OpenAI format validation dataset (20 pairs)
  * Same format and quality as training data
  * Used for validation during fine-tuning process

OUTPUT FILES:
- gpt_oss_arjun_lmstudio/ - Fine-tuned model directory
  * MLX-optimized model files for Apple Silicon
  * LoRA adapters for efficient storage and inference
  * Training configuration and logs

- lm_studio_training_log.txt - Comprehensive training log
  * Training progress, loss curves, validation metrics
  * M4 Max hardware utilization statistics
  * Performance benchmarks and completion status

- training_args.json - MLX-LM training configuration
  * Optimized hyperparameters for M4 Max hardware
  * LoRA configuration for efficient fine-tuning
  * Memory and performance optimization settings

HARDWARE OPTIMIZATION:
- M4 Max 128GB unified memory fully utilized
- Metal Performance Shaders (MPS) acceleration
- Native MXFP4 quantization for memory efficiency
- Tensor parallelism (2-way) for faster training
- Pipeline parallelism (2 stages) for large models

TRAINING FEATURES:
- MLX-LM LoRA fine-tuning for parameter efficiency
- Mixed precision training (bfloat16) for speed
- Dynamic learning rate scheduling with warmup
- Real-time progress monitoring with rich output
- Automatic model conversion and optimization
- Integration with LM Studio for immediate inference

Last Updated: 2025-08-07
Version: 1.0 - LM Studio + MLX Production Ready
"""

import os
import json
import time
import subprocess
import logging
import psutil
import platform
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

# Check dependencies
try:
    import mlx.core as mx
    import mlx_lm
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False

# Apple Silicon Detection
IS_APPLE_SILICON = platform.processor() == 'arm' and platform.system() == 'Darwin'

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• CONFIGURATION â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #

# Model Configuration - Using LM Studio pre-converted MLX model
MODEL_NAME = "lmstudio-community/gpt-oss-20b-MLX-8bit"  # LM Studio MLX model
MODEL_LOCAL_PATH = "gpt_oss_20b_mlx"                    # Local MLX model path
OUTPUT_DIR = "gpt_oss_arjun_lmstudio"  # Fine-tuned model output

# Training Configuration (Optimized for M4 Max + 4-bit MXFP4)
TRAINING_CONFIG = {
    "per_device_train_batch_size": 1,      # Reduced to prevent gradient explosion
    "gradient_accumulation_steps": 16,     # Effective batch size = 64
    "learning_rate": 5e-6,                 # Ultra-low for long sequences stability
    "num_train_epochs": 3,                 # Sufficient for quality fine-tuning
    "warmup_ratio": 0.1,                   # 10% warmup for stability
    "save_steps": 25,                      # Save checkpoints frequently
    "logging_steps": 5,                    # Detailed logging
    "eval_steps": 25,                      # Regular validation
    "max_seq_length": 1024,                # Reduced for numerical stability
    
    # LoRA Configuration (Optimal for 4-bit)
    "lora_r": 16,                          # Optimal rank for 4-bit LoRA
    "lora_alpha": 64,                       # LoRA scaling parameter
    "lora_dropout": 0.1,                    # Regularization
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
    
    # M4 Max Optimizations
    "tp_size": 2,                           # Tensor parallelism
    "pp_size": 1,                           # Pipeline parallelism
    "bf16": True,                           # Mixed precision for Apple Silicon
    "dataloader_num_workers": 4,            # Multi-core data loading
    "remove_unused_columns": False,
    "load_best_model_at_end": True,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": False,
    "save_total_limit": 3,
}

# File Paths
TRAINING_FILE = "arjun_voice_training.jsonl"
VALIDATION_FILE = "arjun_voice_validation.jsonl"
LOG_FILE = "lm_studio_training_log.txt"
CONFIG_FILE = "training_args.json"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• CLASSES â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #

@dataclass
class LMStudioConfig:
    """Configuration for LM Studio + MLX-LM fine-tuning."""
    model_name: str = MODEL_NAME
    model_local_path: str = MODEL_LOCAL_PATH
    output_dir: str = OUTPUT_DIR
    training_config: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.training_config is None:
            self.training_config = TRAINING_CONFIG.copy()

class LMStudioGPTOSSTrainer:
    """
    LM Studio + MLX-LM fine-tuning manager for gpt-oss models.
    
    Optimized for M4 Max hardware with 128GB unified memory,
    Metal Performance Shaders, and Apple Silicon architecture.
    """
    
    def __init__(self, config: LMStudioConfig):
        self.config = config
        self.setup_logging()
        self.start_time = None
        
    def setup_logging(self):
        """Setup comprehensive logging for training monitoring."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(LOG_FILE),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def log_system_info(self):
        """Log detailed M4 Max system information."""
        self.logger.info("ğŸ–¥ï¸  M4 MAX SYSTEM INFORMATION")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ”§ Platform: {platform.platform()}")
        self.logger.info(f"ğŸ§  Processor: {platform.processor()}")
        self.logger.info(f"ğŸ’¾ Total Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB")
        self.logger.info(f"ğŸ”¥ Available Memory: {psutil.virtual_memory().available / (1024**3):.1f} GB")
        self.logger.info(f"âš¡ CPU Cores: {psutil.cpu_count()} logical")
        self.logger.info(f"ğŸ® Apple Silicon: {IS_APPLE_SILICON}")
        self.logger.info(f"ğŸš€ MLX Available: {MLX_AVAILABLE}")
        self.logger.info("=" * 60)
        
    def check_dependencies(self) -> bool:
        """Check if all required dependencies are installed."""
        dependencies = [
            ("mlx", "MLX framework for Apple Silicon"),
            ("mlx_lm", "MLX Language Models for fine-tuning"),
        ]
        
        missing = []
        for dep, desc in dependencies:
            try:
                __import__(dep)
            except ImportError:
                missing.append((dep, desc))
                
        if missing:
            self.logger.error("âŒ Missing dependencies:")
            for dep, desc in missing:
                self.logger.error(f"   - {dep}: {desc}")
            return False
            
        return True
        
    def install_dependencies(self):
        """Install required dependencies for LM Studio + MLX training."""
        self.logger.info("ğŸ“¦ Installing LM Studio + MLX dependencies...")
        
        install_commands = [
            "pip install mlx",
            "pip install mlx-lm",
            "pip install lmstudio",
            "pip install transformers datasets",
            "pip install accelerate",
            "pip install rich tqdm"
        ]
        
        for cmd in install_commands:
            self.logger.info(f"ğŸ”§ Running: {cmd}")
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode != 0:
                self.logger.error(f"âŒ Failed to install: {cmd}")
                self.logger.error(f"Error: {result.stderr}")
            else:
                self.logger.info(f"âœ… Installed: {cmd}")
                
    def download_and_convert_model(self):
        """Download pre-converted MLX model (saves disk space and time)."""
        self.logger.info(f"ğŸ“¥ Using pre-converted MLX model: {self.config.model_name}")
        
        # Check if model already exists locally
        if os.path.exists(self.config.model_local_path):
            self.logger.info(f"âœ… Model already exists at: {self.config.model_local_path}")
            return
            
        try:
            # Download pre-converted MLX model directly
            from huggingface_hub import snapshot_download
            
            self.logger.info(f"ğŸ”„ Downloading pre-converted 4-bit MLX model...")
            self.logger.info(f"ğŸ’¾ This saves disk space and conversion time")
            
            snapshot_download(
                repo_id=self.config.model_name,
                local_dir=self.config.model_local_path,
                local_dir_use_symlinks=False
            )
            
            self.logger.info("âœ… Pre-converted MLX model downloaded successfully")
                
        except Exception as e:
            self.logger.error(f"âŒ Failed to download model: {e}")
            raise
            
    def prepare_dataset(self):
        """Prepare dataset in format compatible with MLX-LM."""
        self.logger.info("ğŸ“ Preparing dataset for MLX-LM training...")
        
        # Load training data
        training_data = []
        with open(TRAINING_FILE, 'r') as f:
            for line in f:
                training_data.append(json.loads(line))
                
        # Load validation data
        validation_data = []
        if os.path.exists(VALIDATION_FILE):
            with open(VALIDATION_FILE, 'r') as f:
                for line in f:
                    validation_data.append(json.loads(line))
        
        self.logger.info(f"ğŸ“Š Training samples: {len(training_data)}")
        self.logger.info(f"ğŸ“Š Validation samples: {len(validation_data)}")
        
        def format_for_mlx(sample):
            """Format sample for MLX-LM training."""
            messages = sample["messages"]
            
            # Build conversation text
            conversation = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                
                if role == "system":
                    conversation += f"System: {content}\\n\\n"
                elif role == "user":
                    conversation += f"Human: {content}\\n\\n"
                elif role == "assistant":
                    conversation += f"Assistant: {content}"
                    
            return {"text": conversation}
        
        # Create MLX dataset directory structure
        mlx_data_dir = "mlx_dataset"
        os.makedirs(mlx_data_dir, exist_ok=True)
        
        mlx_train_file = os.path.join(mlx_data_dir, "train.jsonl")
        mlx_val_file = os.path.join(mlx_data_dir, "valid.jsonl")
        
        # Save training data
        with open(mlx_train_file, 'w') as f:
            for sample in training_data:
                f.write(json.dumps(format_for_mlx(sample)) + '\n')
                
        # Save validation data
        with open(mlx_val_file, 'w') as f:
            for sample in validation_data:
                f.write(json.dumps(format_for_mlx(sample)) + '\n')
                
        self.logger.info(f"âœ… Dataset prepared: {mlx_data_dir}/train.jsonl, {mlx_data_dir}/valid.jsonl")
        return mlx_data_dir, mlx_val_file
        
    def create_training_config(self):
        """Create training configuration file for MLX-LM."""
        self.logger.info("ğŸ”§ Creating training configuration...")
        
        # Save training configuration
        with open(CONFIG_FILE, 'w') as f:
            json.dump(self.config.training_config, f, indent=2)
            
        self.logger.info(f"âœ… Training config saved: {CONFIG_FILE}")
        
        # Log key configuration parameters
        config = self.config.training_config
        self.logger.info("ğŸ“‹ Key Training Parameters:")
        self.logger.info(f"   ğŸ¯ Learning Rate: {config['learning_rate']}")
        self.logger.info(f"   ğŸ“¦ Batch Size: {config['per_device_train_batch_size']}")
        self.logger.info(f"   ğŸ”„ Epochs: {config['num_train_epochs']}")
        self.logger.info(f"   ğŸ§  LoRA Rank: {config['lora_r']}")
        self.logger.info(f"   âš¡ Tensor Parallel: {config['tp_size']}")
        
    def start_fine_tuning(self, train_file: str, val_file: str):
        """Start MLX-LM fine-tuning process."""
        self.logger.info("ğŸš€ Starting MLX-LM fine-tuning...")
        self.logger.info(f"ğŸ¯ Target: Arjun's investment voice with premium Claude dataset")
        
        self.start_time = time.time()
        
        try:
            # Build MLX-LM LoRA command (correct syntax) - use shell command for conda activation
            lora_cmd = f"""source /opt/anaconda3/etc/profile.d/conda.sh && conda activate mlx_clean && python -m mlx_lm lora \
                --model {self.config.model_local_path} \
                --data mlx_short_dataset \
                --train \
                --adapter-path {self.config.output_dir} \
                --iters {self.config.training_config["num_train_epochs"] * 100} \
                --learning-rate {self.config.training_config["learning_rate"]} \
                --batch-size {self.config.training_config["per_device_train_batch_size"]} \
                --num-layers {self.config.training_config["lora_r"]} \
                --save-every {self.config.training_config["save_steps"]} \
                --val-batches 10 \
                --max-seq-length {self.config.training_config["max_seq_length"]} \
                --grad-checkpoint"""
            
            # Execute training command
            self.logger.info(f"ğŸ”„ MLX-LM Command: {lora_cmd}")
            self.logger.info(f"ğŸ’¾ Memory before training: {psutil.virtual_memory().used / (1024**3):.1f} GB used")
            self.logger.info("ğŸ“Š Training Progress:")
            
            # Run the command and stream output (using shell=True for conda activation)
            process = subprocess.Popen(
                lora_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True,
                shell=True  # Required for conda activation
            )
            
            # Monitor training progress
            for line in process.stdout:
                line = line.strip()
                if line:
                    self.logger.info(f"   {line}")
                    
            # Wait for completion
            process.wait()
            
            if process.returncode == 0:
                training_time = time.time() - self.start_time
                
                # Monitor memory after training
                memory_after = psutil.virtual_memory()
                self.logger.info(f"ğŸ’¾ Memory after training: {memory_after.used / (1024**3):.1f} GB used")
                
                self.logger.info(f"âœ… Training completed in {training_time:.2f} seconds ({training_time/60:.1f} minutes)")
                self.logger.info(f"ğŸ’¾ Model saved to: {self.config.output_dir}")
                
                # Test the fine-tuned model
                self.test_fine_tuned_model()
                
            else:
                self.logger.error(f"âŒ Training failed with return code: {process.returncode}")
                raise RuntimeError("MLX-LM training failed")
                
        except Exception as e:
            self.logger.error(f"âŒ Fine-tuning failed: {e}")
            raise
            
    def test_fine_tuned_model(self):
        """Test the fine-tuned model with a sample prompt."""
        self.logger.info("ğŸ§ª Testing fine-tuned model...")
        
        test_prompt = "What are the key factors to consider when investing in emerging markets?"
        
        try:
            # Generate response using MLX-LM
            generate_cmd = [
                "python", "-m", "mlx_lm.generate",
                "--model", self.config.model_local_path,
                "--adapter-path", self.config.output_dir,
                "--prompt", test_prompt,
                "--max-tokens", "200",
                "--temp", "0.7"
            ]
            
            self.logger.info(f"ğŸ”„ Testing with prompt: {test_prompt}")
            
            result = subprocess.run(generate_cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                response = result.stdout.strip()
                self.logger.info("âœ… Model test successful!")
                self.logger.info(f"ğŸ“ Response: {response}")
            else:
                self.logger.warning(f"âš ï¸ Model test failed: {result.stderr}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ Could not test model: {e}")
            
    def run_training_pipeline(self):
        """Execute complete LM Studio + MLX training pipeline."""
        self.logger.info("ğŸš€ STARTING LM STUDIO + MLX GPT-OSS FINE-TUNING")
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"ğŸ§  Model: {self.config.model_name}")
        self.logger.info(f"ğŸ“Š Dataset: Premium Claude Opus processed")
        self.logger.info(f"âš¡ Method: MLX-LM LoRA with M4 Max optimization")
        self.logger.info(f"ğŸ® Platform: LM Studio + Apple Silicon")
        self.logger.info("=" * 70)
        
        try:
            # Log system information
            self.log_system_info()
            
            # Check dependencies
            if not self.check_dependencies():
                self.install_dependencies()
                
            # Download and convert model
            self.download_and_convert_model()
            
            # Prepare dataset
            train_file, val_file = self.prepare_dataset()
            
            # Create training configuration
            self.create_training_config()
            
            # Start fine-tuning
            self.start_fine_tuning(train_file, val_file)
            
            self.logger.info("ğŸ‰ LM STUDIO + MLX TRAINING COMPLETED SUCCESSFULLY!")
            self.logger.info(f"ğŸ“ Model location: {self.config.output_dir}")
            self.logger.info("ğŸš€ Ready for LM Studio inference and testing")
            self.logger.info("ğŸ’¡ Load in LM Studio: File â†’ Load Model â†’ Select adapter folder")
            
        except Exception as e:
            self.logger.error(f"âŒ Training pipeline failed: {e}")
            raise

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• MAIN EXECUTION â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #

def main():
    """Main execution function."""
    print("ğŸš€ LM STUDIO + MLX GPT-OSS FINE-TUNING MANAGER")
    print("=" * 70)
    print(f"ğŸ§  Model: OpenAI gpt-oss-20b (Released Aug 5, 2025)")
    print(f"ğŸ–¥ï¸  Hardware: M4 Max with 128GB unified memory")
    print(f"ğŸ“Š Dataset: Premium Claude Opus processed (100 pairs)")
    print(f"âš¡ Method: MLX-LM LoRA with Apple Silicon optimization")
    print(f"ğŸ® Platform: LM Studio + Metal Performance Shaders")
    print("=" * 70)
    
    # Initialize configuration
    config = LMStudioConfig()
    
    # Create trainer and run
    trainer = LMStudioGPTOSSTrainer(config)
    trainer.run_training_pipeline()

if __name__ == "__main__":
    main()
